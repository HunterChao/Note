统计语言模型

1. 表述
2. 简单数学描述
3. 实际处理
    - 高阶语言模型
    - 模型的训练、零概率问题和平滑方法

# 1.表述
*统计语言模型*是所有自然语言处理的基础，广泛应用于机器翻译、语音识别、印刷体或手写体
识别、拼写纠错、汉字输入和文献查询。

# 2.简单数学描述
统计语言模型产生的初衷是为了解决*语音识别*问题。在语音识别中，计算机需要知道*一个
文字序列是否能构成一个大家理解而且有意义的句子*。上世纪70年代以前，科学家们试图
判断这个文字序列是否合乎文法、语义是否正确等，但由于句子结构可能非常复杂，这条路
似乎走不通。统计语言模型的出发点很简单：

```
一个句子是否合理，就看它的概率大小如何。
```
普遍而严格的表述：
假定`S`表示某一个有意义的句子，由一连串特定顺序排列的词`W1,W2,...,Wn`组成，这里`n`是句子的长度。
我们想知道的`P(S)`的概率可以表示为`P(s) = P(W1,W2,...,Wn)`。利用条件概率公式
，我们得到：![条件概率](http://i.imgur.com/sGeMTmr.png)。`P(W2|W1)`表示在已知第一个词
出现的前提下，第二个词出现的概率。所以我们会知道，词`Wn`出现的概率取决于它前面的所有词。

从计算上来看，`P(W1)`比较容易计算,`P(W2|W1)`还不太麻烦,`P(W3|W1,W2)`就开始变得难算了，
因为它涉及到了三个变量，每个变量的可能取值都是所有词。到最后一个条件概率我们就根本无法计算了。

为此，*马尔可夫假设*被提出:`假设任意一个词Wi出现的频率只同它前面的词Wi-1有关`。
于是我们的`P(S)`就可以写为:

![Imgur](http://i.imgur.com/eVSVCKh.png)

上式对应的统计语言模型是*二元模型*（Bigram Model）。当然我们可以假设一个词由前面
`N-1`个词决定，对应的模型被称为*N元模型*，这个稍后讨论。

接下来的问题是如何估计条件概率：`P(Wi|Wi-1)`，根据定义：

![条件概率](http://i.imgur.com/zgYCdG9.png)

我们只用估计联合概率（分子）和边缘概率（分母）即可。根据大数定理，只要统计量
足够，我们可以用频率代替概率。所以我们只需统计出单词`Wi-1`和二元组`Wi-1，Wi`的频率即可。

# 3.实际处理

## （1）高阶语言模型
假定文本中的每个词`Wi`和前面`N-1`个词有关，而与更前面的词无关,这样当前词`Wi`的
条件概率可以表示为：

![hh](http://i.imgur.com/3gDhuqT.png)

这种假设被称为*N-1阶马尔可夫假设*，对应的语言模型被称为*N元模型*（N-Gram Model）。
`N`为2时为上节介绍的二元模型，`N`为1时是一个上下文无关模型。实际中应用最多的是`N=3`的三元模型。

<br />

*为何N的取值一般很小*

1. N元模型的空间复杂度是N的指数函数，即`O(|V|^N)`,其中`|V|`为一种语言词典的词汇量。
它的时间复杂度也是指数级，即`O(|V|^(N-1))`（Why？）.
2. 当N从1到2，从2到3，模型的效果上升显著。但当N从3到4时，效果的提升就不是很显著，但
资源的消耗却增加得非常快。

<br />

*三元、四元甚至更高阶的模型是不是就能覆盖所有语言现象呢*

答案当然不是。在自然语言中，上下文的相关性可能跨域非常大，甚至可能跨段落。这种情况下
再提高模型也无济于事，这是马尔可夫假设的局限性，此时应采取其他一些*长程的依赖性*
（Long Distance Dependency）来解决这个问题。

## （2）模型的训练、零概率问题和平滑方法

**模型的训练**

语言模型的参数就是模型中所有的条件概率。通过对语料的统计，得到这些参数的过程
称为模型的训练。

<br />

**零概率问题**

上节，我们已知求解二元模型`P(S)`，我们必须知道二元组`(Wi-1，Wi)`和词`Wi-1`的频数。但是
当元组`(Wi-1， Wi)`的频数为0时，是否意味着条件概率`P(Wi|Wi-1)=0`？如果二元组
`(Wi-1, Wi)`和`Wi-1`频数都为1，能否说明`P(Wi|Wi-1)=1`？显然并不是，因为此时观测值
不足，*频率等于概率*并不被*大数定理*支持。

我们可以使用*增加数据*的方法，然而零概率依然会出现。如果模型的大部分条件概率为零，我们
称这种现象为*不平滑*。统计模型的零概率问题无法回避。

为了解决零概率问题，古德在图灵指导下，提出了一种*相信可靠的统计数据，对不可信统计数据打折扣*
的一种概率估计方法。它将折扣出来的一小部分概率给予未看见的事件。

*古德--图灵估计*(Good–Turing frequency estimation)

描述：对于没有看见的事件，我们不能认为它发生的概率是零，因此我们需要从概率的总量
（Probability Mass）中，分配一个很小的比例给这些没有看见的事件。（如下图）

![古德--图灵估计](http://images.cnitblog.com/blog/408927/201412/202122413296010.png)

以统计词典中每个词的频率为例来说明古德--图灵估计公式。
假定在语料库中出现`r`次的词有`Nr`个，特别地，未出现的词数量为![Imgur](http://i.imgur.com/3rrkMv2.png)。
语料库的大小为`N`，那么很显然：

![Imgur](http://i.imgur.com/APkOIM6.png)  (1)

出现`r`次的词在整个语料库上的相对频度（Relative Frequency）则是`r/N`，如果不做
任何处理，就以这个相对频度作为这些词的概率估计。

现假定当`r`比较小时，它的统计可能不可靠（不能根据大数定律），因此在计算这些词的概率时，
要使用一个更小一点的频数`dr`，而不直接使用`r`。

古德--图灵估计按照下面的公式计算`dr`:

![Imgur](http://i.imgur.com/RsI1gMM.png) (2)

显然：

![Imgur](http://i.imgur.com/wFcOQd0.png) (3)

（将（2）式代入（3）式左部，再根据（1）式，即可得到（3）式右部）

一般来说，出现一次的词的数量比出现两次的多，出现两次的比三次的多。这种规律称为
*Zipf定律*（Zipf’s Law）。下图是一个小语料上出现r次的词的数量`Nr`和`r`的关系。

![](http://images.cnitblog.com/blog/408927/201412/202211207981424.png)

当`r=0`时，根据公式（2）,`d0>0`。这样就给为出现的词赋予了一个很小的非零值，
从而解决了零概率的问题。同时下调了出现概率很低的词的概率。

在实际的自然语言处理中，一般会设置一个阈值T，仅对出现次数小于T的词做上述调整。
这样，对于频率超过一定阈值的词，它们的概率估计就是它们在语料库中的相对频度；
对于频率小于这个阈值的词，它们的频率就小于它们的相对频度，出现次数越少折扣越多；
对于未看见的词，也给予了一个比较小的频率。这样，所有词的概率估计就很*平滑*了。

*在二元模型运用平滑*

![Imgur](http://i.imgur.com/7DNaZgM.jpg)

这种平滑的方法，最早由前IBM 科学家卡茨（S. M. Katz) 提出，故称为卡茨退避法
（Katz backoff）。类似地，对于三元模型，概率估计的公式如下：

![Imgur](http://i.imgur.com/sC2TNej.jpg)

<br />

*线性插值法*

用低阶语言模型和高阶模型进行线性插值来达到平滑的目的， 也是过去行业使用的一
种方法， 这种方法称为删除差值（Deleted Interpolation），如下面的公式。该公
式中的三个 λ 均为正数而且和为1。线性插值的效果略低于卡茨退避法（Backoff），
故现在已经较少使用了。

![Imgur](http://i.imgur.com/2hZkOkj.jpg)





