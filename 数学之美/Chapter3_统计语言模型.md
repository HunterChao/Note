统计语言模型

1. 表述
2. 简单数学描述
3. 实际处理
    - 高阶语言模型
    - 模型的训练、零概率问题和平滑方法

# 1.表述
*统计语言模型*是所有自然语言处理的基础，广泛应用于机器翻译、语音识别、印刷体或手写体
识别、拼写纠错、汉字输入和文献查询。

# 2.简单数学描述
统计语言模型产生的初衷是为了解决*语音识别*问题。在语音识别中，计算机需要知道*一个
文字序列是否能构成一个大家理解而且有意义的句子*。上世纪70年代以前，科学家们试图
判断这个文字序列是否合乎文法、语义是否正确等，但由于句子结构可能非常复杂，这条路
似乎走不通。统计语言模型的出发点很简单：

```
一个句子是否合理，就看它的概率大小如何。
```
普遍而严格的表述：
假定`S`表示某一个有意义的句子，由一连串特定顺序排列的词`W1,W2,...,Wn`组成，这里`n`是句子的长度。
我们想知道的`P(S)`的概率可以表示为`P(s) = P(W1,W2,...,Wn)`。利用条件概率公式，
，我们得到：![条件概率](http://i.imgur.com/sGeMTmr.png)。`P(W2|W1)`表示在已知第一个词
出现的前提下，第二个词出现的概率。所以我们会知道，词`Wn`出现的概率取决于它前面的所有词。

从计算上来看，`P(W1)`比较容易计算,`P(W2|W1)`还不太麻烦,`P(W3|W1,W2)`就开始变得难算了，
因为它涉及到了三个变量，每个变量的可能取值都是所有词。到最后一个条件概率我们就根本无法计算了。

为此，马尔可夫*马尔可夫假设*:`假设任意一个词Wi出现的频率只同它前面的词Wi-1有关`。
于是我们的`P(S)`就可以写为:

![Imgur](http://i.imgur.com/eVSVCKh.png)

上式对应的统计语言模型是*二元模型*（Bigram Model）。当然我们可以假设一个词由前面
`N-1`个词决定，对应的模型被称为*N元模型*，这个稍后讨论。

接下来的问题是如何估计条件概率：`P(Wi|Wi-1)`，根据定义：

![条件概率](http://i.imgur.com/zgYCdG9.png)

我们只用估计联合概率（分子）和边缘概率（分母）即可。根据大数定理，只要统计量
足够，我们可以用频率代替概率。所以我们只需统计出单词`Wi-1`和二元组`Wi-1，Wi`的频率即可。

# 3.实际处理

## （1）高阶语言模型
假定文本中的每个词`Wi`和前面`N-1`个词有关，而与更前面的词无关,这样当前词`Wi`的
条件概率可以表示为：

![hh](http://i.imgur.com/3gDhuqT.png)

这种假设被称为*N-1阶马尔可夫假设*，对应的语言模型被称为*N元模型*（N-Gram Model）。
`N`为2时为上节介绍的二元模型，`N`为1时是一个上下文无关模型。实际中应用最多的是`N=3`的三元模型。

<br />

*为何N的取值一般很小*

1. N元模型的空间复杂度是N的指数函数，即`O(|V|^N)`,其中`|V|`为一种语言词典的词汇量。
它的时间复杂度也是指数级，即`O(|V|^(N-1))`（Why？）.
2. 当N从1到2，从2到3，模型的效果上升显著。但当N从3到4时，效果的提升就不是很显著，但
资源的消耗却增加得非常快。

<br />

*三元、四元甚至更高阶的模型是不是就能覆盖所有语言现象呢*

答案当然不是。在自然语言中，上下文的相关性可能跨域非常大，甚至可能跨段落。这种情况下
再提高模型也无济于事，这是马尔可夫假设的局限性，此时应采取其他一些*长程的依赖性*
（Long Distance Dependency）来解决这个问题。

## （2）模型的训练、零概率问题和平滑方法


