目录
1. 什么是类不平衡
2. 为什么要对*类不平衡*特殊处理
2. 提升不平衡类分类准确率的方法
    - 采样（Oversampling，Undersampling）
    - 阈值移动（Threshold Moving）
    - 调整代价或权重法
3. 方法评价

# 1.什么是类不平衡
> Given **two-class** data, the data are class-imbalanced if the main class of interest (the
*positive class*) is represented by only a few tuples, while the majority of tuples represent
the negative class.
>
> For **multiclass**-imbalanced data, the data distribution of each class
differs substantially where, again, the main class or classes of interest are rare.
>
>The class imbalance problem is closely related to *cost-sensitive learning*, wherein the costs of
errors, per class, are not equal.

# 2.为什么要对*类不平衡*作特殊处理
> Traditional classification algorithms aim to minimize the number of errors made during classification.
 They assume that the costs of *false positive* and *false negative* errors
are equal. By assuming a balanced distribution of classes and equal error costs, they
are therefore not suitable for class-imbalanced data.

# 3.提升不平衡类分类准确率的方法
> These approaches include  **oversampling**, **undersampling**, **threshold moving**, and **ensemble techniques**.
>
>The first three do not
involve any changes to the construction of the classification model. That is, *oversampling* and *undersampling* change the distribution of tuples in the training set;
*threshold moving* affects how the model makes decisions when classifying new data.

## （1）采样

>Both oversampling and undersampling change the training data distribution so that
the rare (positive) class is well represented.

**上采样(Oversampling)**
> Oversampling works by resampling the positive tuples so that the resulting training set contains an equal number of positive and
negative tuples.

<br />

**下采样(Undersampling)**
> Undersampling works by decreasing the number of negative tuples. It
randomly eliminates tuples from the majority (negative) class until there are an equal
number of positive and negative tuples.

## （2）阈值移动
> The threshold-moving approach to the class imbalance problem does not involve
any sampling. It applies to classifiers that, given an input tuple, return a continuous
output value.
That is, for an input tuple, `X`, such a classifier returns as output a mapping,
`f(X)->[0,1]`.
>
>Rather than manipulating the training tuples, this method returns a classification decision based on the output values.
> - In the simplest approach, tuples for which
`f(X)≥t`, for some threshold, `t`, are considered positive, while all other tuples are considered negative.
> - Other approaches may involve manipulating the outputs by weighting.
In general, *threshold moving* moves the threshold, `t`, so that the rare class tuples are easier to classify (and hence, there is less chance of costly false negative errors).
>
>Examples of
such classifiers include **naive Bayesian classifiers** and **neural network classifiers** like backpropagation.

（Question：朴素贝叶斯分类和反向传播神经网络分类不需要把类不平衡特殊考虑么？）

## （3）调整代价或权重法

NULL

# 3.方法评价

**在二分类任务中**

>These methods work relatively well for the class imbalance problem on two-class
tasks. *Threshold-moving* and *ensemble methods* were empirically observed to outperform *oversampling* and *undersampling*.
*Threshold moving* works well even on data sets that are extremely imbalanced.

<br />

**在多分类任务中**
> The class imbalance problem on multiclass tasks
is much more difficult, where oversampling and threshold moving are less effective.
Although threshold-moving and ensemble methods show promise, finding a solution
for the multiclass imbalance problem remains an area of future work.

<br />
<br />

**Ref**

[Data Mining: Concepts and Techniques (Jiawei Han)]()
