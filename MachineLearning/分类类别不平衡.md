目录

1. 什么是类不平衡
2. 为什么要对*类不平衡*特殊处理
2. 提升不平衡类分类准确率的方法
    - 采样（Oversampling，Undersampling）
    - 阈值移动（Threshold Moving）
    - 调整代价或权重法
3. 方法评价

# 1.什么是类不平衡
> Given **two-class** data, the data are class-imbalanced if the main class of interest (the
*positive class*) is represented by only a few tuples, while the majority of tuples represent
the negative class.
>
> For **multiclass**-imbalanced data, the data distribution of each class
differs substantially where, again, the main class or classes of interest are rare.
>
>The class imbalance problem is closely related to **cost-sensitive learning**, wherein the costs of
errors, per class, are not equal.

# 2.为什么要对*类不平衡*作特殊处理
> Traditional classification algorithms aim to minimize the number of errors made during classification.
 They assume that the costs of *false positive* and *false negative* errors
are equal. By assuming a balanced distribution of classes and equal error costs, they
are therefore not suitable for class-imbalanced data.

# 3.提升不平衡类分类准确率的方法
> These approaches include  **oversampling**, **undersampling**, **threshold moving**, and **ensemble techniques**.
>
>The first three do not
involve any changes to the construction of the classification model. That is, *oversampling* and *undersampling* change the distribution of tuples in the training set;
*threshold moving* affects how the model makes decisions when classifying new data.

## （1）采样

>Both oversampling and undersampling change the training data distribution so that
the rare (positive) class is well represented.

**上采样(Oversampling)**
> Oversampling works by resampling the positive tuples so that the resulting training set contains an equal number of positive and
negative tuples.

**注意**：过采样不能简单地对初始正例样本进行重复采样，否则会产生严重的过拟合；
过采样法的代表性算法SMOTE是通过对训练集里的正例进行插值来产生额外的正例。

[点击--> `SMOTE: Synthetic Minority Over-sampling Technique`](http://jair.org/media/953/live-953-2037-jair.pdf)

<br />

**下采样(Undersampling)**
> Undersampling works by decreasing the number of negative tuples. It
randomly eliminates tuples from the majority (negative) class until there are an equal
number of positive and negative tuples.

**注意**：
欠采样若随机丢弃反例，可能丢失一些很重要的信息；欠采样的代表性算法EasyEnsemble则是利用集成
学习机制，将反例划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在
全局来看却不会丢失重要信息。

[点击--> `Exploratory Undersampling for Class-Imbalance Learning`](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tsmcb09.pdf)

## （2）阈值移动
> The threshold-moving approach to the class imbalance problem does not involve
any sampling. It applies to classifiers that, given an input tuple, return a continuous
output value.
That is, for an input tuple, `X`, such a classifier returns as output a mapping,
`f(X)->[0,1]`.
>
>Rather than manipulating the training tuples, this method returns a classification decision based on the output values.
> - In the simplest approach, tuples for which
`f(X)≥t`, for some threshold, `t`, are considered positive, while all other tuples are considered negative.
> - Other approaches may involve manipulating the outputs by weighting.
In general, *threshold moving* moves the threshold, `t`, so that the rare class tuples are easier to classify (and hence, there is less chance of costly false negative errors).
>
>Examples of
such classifiers include **naive Bayesian classifiers** and **neural network classifiers** like backpropagation.

我们从线性分类器的角度理解，在我们用`y = wx + b`对新样本进行分类时，实际上是在用预测出的
`y`值与一个阈值进行比较，例如通常在 `y > 0.5`时判别为正例，否则为反例。`y`实际上表达了正例
的可能性，几率`y/(1-y)`则反映了正例可能性与反例可能性之比，阈值设为`0.5`表明分类器认为真实
正、反例可能性相同，即分类器决策规则为：

`若 y/(1-y) > 1, 则预测为正例`

当训练集中正反例数目不同时，令`m+`表示正例数目，`m-`表示反例数目，则观测几率是`m+ / m-`,
由于通常我们假设训练集是真实样本总体的无偏采样，因此观测几率就代表了真实几率。于是，
只要分类器的预测几率高于观测几率就应该判定为正例，即

`若 y/(1-y) > m+/m- ,则预测为正例`

等价于我们 **再缩放(rescaling)** 了`y`为`y'`,即

`若 y'/(1-y') = (y/(1-y))*(m-/m+) > 1 ， 则预测为正例`

**注意**：

1. 再次强调，这种方法是直接基于原始训练集进行学习，但在用训练好的分类器进行预测时，使用
上式做决策。
2. 再缩放实际操作起来并不见得有多好，主要是假设“训练集是真实样本总体的无偏采样”往往并不成立。

## （3）调整代价或权重法

NULL

# 3.方法评价

**在二分类任务中**

>These methods work relatively well for the class imbalance problem on two-class
tasks. *Threshold-moving* and *ensemble methods* were empirically observed to outperform *oversampling* and *undersampling*.
*Threshold moving* works well even on data sets that are extremely imbalanced.

<br />

**在多分类任务中**
> The class imbalance problem on multiclass tasks
is much more difficult, where oversampling and threshold moving are less effective.
Although threshold-moving and ensemble methods show promise, finding a solution
for the multiclass imbalance problem remains an area of future work.

<br />
<br />

**More Info**

[A Brief Literature Review of Class Imbalanced Problem](http://www.cs.cmu.edu/~qyj/IR-Lab/ImbalancedSummary.html)

<br />

**Ref**

[Data Mining: Concepts and Techniques (Jiawei Han)]()

[机器学习 （周志华）]()