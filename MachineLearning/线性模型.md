目录：
1. 线性模型简介
2. 线性回归
    - 原理
    - 代码示例
    - 缺点
    - 复杂度分析
3. 岭回归
    - 原理
    - 代码示例
    - 复杂度分析
    - 设置alpha参数
4. Lasso
    - 原理
    - 代码示例
    - 设置alpha参数

# 1. 线性模型简介
线性模型的target value是关于输入变量的线性组合。用数学公式表达就是：

![](http://scikit-learn.org/stable/_images/math/334dd847bce79ed52a760f02b3efd8faefdb6e8b.png)

![](http://scikit-learn.org/stable/_images/math/843835b028b47682b57b538b10dc03336f27a34d.png)被称作系数,在sklearn标记为`coef_`; ![](http://scikit-learn.org/stable/_images/math/faafb19d9d95d41a5554bd1b8d8ca2e16a89935d.png)被称作截距，在sklearn中标记为`intercept_`。

# 2. 线性回归
## （1）原理
LinearRegression fits a linear model with coefficients `w = (w_1, ..., w_p)` to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. Mathematically it solves a problem of the form:

![](http://scikit-learn.org/stable/_images/math/e8e92a5482d9327d939e7a17946a8a1b98006018.png)

## （2）代码示例
```python
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,  0.5])
>>> reg.intercept_
2.2204460492503131e-16
```
## （3）缺点
线性回归使用**普通最小二乘法**进行参数估计，这种估计方法依赖于特征间的独立性。当特征之间有近似的线性依赖时，特征矩阵趋向于奇异矩阵，结果导致最小二乘估计对于观测到的target value的随机误差很敏感，容易造成很大的方差（variance）。这里引用我另一篇笔记[FeatureSelection](https://github.com/wangjiang0624/Note/blob/master/MachineLearning/FeatureSelection.md)中的一个例子：
> Q:特征之间存在较大的相关性会对模型的结果有影响吗？

> A:会有。在很多实际的数据当中，往往存在多个互相关联的特征，这时候模型就会变得不稳定，数据中细微的变化就可能导致模型的巨大变化（模型的变化本质上是系数，或者叫参数，可以理解成`W`），这会让模型的预测变得困难，这种现象也称为多重共线性。例如，假设我们有个数据集，它的真实模型应该是`Y=X1+X2`，当我们观察的时候，发现`Y’=X1+X2+e`，`e`是噪音。如果`X1`和`X2`之间存在线性关系，例如`X1`约等于`X2`，这个时候由于噪音`e`的存在，我们学到的模型可能就不是`Y=X1+X2`了，有可能是`Y=2X1`，或者`Y=-X1+3X2`。
## （4）复杂度分析
计算最小二乘使用特征矩阵`X`的奇异值分解。假设`X`为shape为`n*p`，假设`n>=p`,则时间复杂度为![](http://scikit-learn.org/stable/_images/math/ed6ddbbf57a29a1b9e7a4afd3898df76d7cff16e.png)。

# 2. 岭回归（Ridge Regression）
## （1）原理
岭回归通过引入了一个系数复杂度的罚项，来解决普通最小二乘在变量间出现多重共线问题时的不足。系数(权重)复杂度可以理解为不同系数分布的离散程度。岭回归最小化一个惩罚过的残差平方和：

![](http://scikit-learn.org/stable/_images/math/48dbdad39c89539c714a825c0c0d5524eb526851.png)

这里alpha是一个复杂度参数，它控制系数分布的收缩程度：alpha越大，收缩程度越大(系数分布越紧凑)，因此系数对于共线性就越稳定。看下面岭迹图就明白了：

![](http://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png)

上图：每一条线表示一个系数（或参数）的值随着alpha的变化而变化。有一些参数在alpha较小时波动较大，很可能就是多重共线造成的。所以我们在选择alpha时，要选择系数都比较稳定时的alpha值。

## （2）代码示例
```python
>>> from sklearn import linear_model
>>> reg = linear_model.Ridge (alpha = .5)
>>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
>>> reg.coef_
array([ 0.34545455,  0.34545455])
>>> reg.intercept_ 
0.13636...
```

## （3）复杂度分析
和普通线性回归有相同阶的时间复杂度。

## （4）设置alpha参数
[RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV)实现了基于交叉验证的岭回归参数优化，用法示例：
```python
>>> from sklearn import linear_model
>>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
    normalize=False)
>>> reg.alpha_                                      
0.1
```

# 4. Lasso
## （1）原理
Lasso是一个用来估计稀疏系数的线性模型。它产生的系数解具有稀疏性，当特征间有依赖关系时，可以有效地减少特征的个数。例如Lasso和它的变体是压缩感知的基础。它的目标函数是：

![](http://scikit-learn.org/stable/_images/math/07c30d8004d4406105b2547be4f3050048531656.png)

`||w||1`是参数向量w的L1范数。

## （2）代码示例
```python
>>> from sklearn import linear_model
>>> reg = linear_model.Lasso(alpha = 0.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
>>> reg.predict([[1, 1]])
array([ 0.8])
```
## （3）设置alpha参数
参数alpha控制被估计的系数的稀疏程度。sklearn提供了三种得到alpha参数的方法：
[LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)、[LassoLarsCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV)、[LassoLarsIC](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC)

对于带有许多共线回归量的高维数据集，LassoCV效果更好。

[More Info——>Setting regularization parameter](http://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter)