目录：
1. 线性模型简介
2. 线性回归
    - 原理
    - 代码示例
    - 缺点
    - 复杂度分析
3. 岭回归
    - 原理
    - 代码示例
    - 复杂度分析
    - 设置alpha参数
4. Lasso
    - 原理
    - 代码示例
    - 设置alpha参数
    - 多任务Lasso
5. Elastic Net
    - 原理
    - 设置参数
    - 多任务ElasticNet
6. 逻辑回归
    - 原理
    - 与最大熵模型关系
7. 多项式回归
    - 原理
    - 代码示例
8. 线性（二次）判别分析
    - 原理
    - 代码示例
    - Shrinkage


# 1. 线性模型简介
线性模型的target value是关于输入变量的线性组合。用数学公式表达就是：

![](http://scikit-learn.org/stable/_images/math/334dd847bce79ed52a760f02b3efd8faefdb6e8b.png)

![](http://scikit-learn.org/stable/_images/math/843835b028b47682b57b538b10dc03336f27a34d.png)被称作系数,在sklearn标记为`coef_`; ![](http://scikit-learn.org/stable/_images/math/faafb19d9d95d41a5554bd1b8d8ca2e16a89935d.png)被称作截距，在sklearn中标记为`intercept_`。

# 2. 线性回归
## （1）原理
[LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) fits a linear model with coefficients `w = (w_1, ..., w_p)` to minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear approximation. Mathematically it solves a problem of the form:

![](http://scikit-learn.org/stable/_images/math/e8e92a5482d9327d939e7a17946a8a1b98006018.png)

## （2）代码示例
```python
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit ([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> reg.coef_
array([ 0.5,  0.5])
>>> reg.intercept_
2.2204460492503131e-16
```
## （3）缺点
线性回归使用**普通最小二乘法**进行参数估计，这种估计方法依赖于特征间的独立性。当特征之间有近似的线性依赖时，特征矩阵趋向于奇异矩阵，结果导致最小二乘估计对于观测到的target value的随机误差很敏感，容易造成很大的方差（variance）。这里引用我另一篇笔记[FeatureSelection](https://github.com/wangjiang0624/Note/blob/master/MachineLearning/FeatureSelection.md)中的一个例子：
> Q:特征之间存在较大的相关性会对模型的结果有影响吗？

> A:会有。在很多实际的数据当中，往往存在多个互相关联的特征，这时候模型就会变得不稳定，数据中细微的变化就可能导致模型的巨大变化（模型的变化本质上是系数，或者叫参数，可以理解成`W`），这会让模型的预测变得困难，这种现象也称为多重共线性。例如，假设我们有个数据集，它的真实模型应该是`Y=X1+X2`，当我们观察的时候，发现`Y’=X1+X2+e`，`e`是噪音。如果`X1`和`X2`之间存在线性关系，例如`X1`约等于`X2`，这个时候由于噪音`e`的存在，我们学到的模型可能就不是`Y=X1+X2`了，有可能是`Y=2X1`，或者`Y=-X1+3X2`。
## （4）复杂度分析
计算最小二乘使用特征矩阵`X`的奇异值分解。假设`X`为shape为`n*p`，假设`n>=p`,则时间复杂度为![](http://scikit-learn.org/stable/_images/math/ed6ddbbf57a29a1b9e7a4afd3898df76d7cff16e.png)。

# 2. 岭回归（Ridge Regression）
## （1）原理
[Ridge Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)岭回归通过引入了一个系数复杂度的罚项，来解决普通最小二乘在变量间出现多重共线问题时的不足。系数(权重)复杂度可以理解为不同系数分布的离散程度。岭回归最小化一个惩罚过的残差平方和：

![](http://scikit-learn.org/stable/_images/math/48dbdad39c89539c714a825c0c0d5524eb526851.png)

这里alpha是一个复杂度参数，它控制系数分布的收缩程度：alpha越大，收缩程度越大(系数分布越紧凑)，因此系数对于共线性就越稳定。看下面岭迹图就明白了：

![](http://scikit-learn.org/stable/_images/sphx_glr_plot_ridge_path_001.png)

上图：每一条线表示一个系数（或参数）的值随着alpha的变化而变化。有一些参数在alpha较小时波动较大，很可能就是多重共线造成的。所以我们在选择alpha时，要选择系数都比较稳定时的alpha值。

## （2）代码示例
```python
>>> from sklearn import linear_model
>>> reg = linear_model.Ridge (alpha = .5)
>>> reg.fit ([[0, 0], [0, 0], [1, 1]], [0, .1, 1]) 
Ridge(alpha=0.5, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
>>> reg.coef_
array([ 0.34545455,  0.34545455])
>>> reg.intercept_ 
0.13636...
```

## （3）复杂度分析
和普通线性回归有相同阶的时间复杂度。

## （4）设置alpha参数
[RidgeCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV)实现了基于交叉验证的岭回归参数优化，用法示例：
```python
>>> from sklearn import linear_model
>>> reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])
>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])       
RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, scoring=None,
    normalize=False)
>>> reg.alpha_                                      
0.1
```

# 4. Lasso
## （1）原理
[Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)是一个用来估计稀疏系数的线性模型。它产生的系数解具有稀疏性，当特征间有依赖关系时，可以有效地减少特征的个数。例如Lasso和它的变体是压缩感知的基础。它的目标函数是：

![](http://scikit-learn.org/stable/_images/math/07c30d8004d4406105b2547be4f3050048531656.png)

`||w||1`是参数向量w的L1范数。

## （2）代码示例
```python
>>> from sklearn import linear_model
>>> reg = linear_model.Lasso(alpha = 0.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False, random_state=None,
   selection='cyclic', tol=0.0001, warm_start=False)
>>> reg.predict([[1, 1]])
array([ 0.8])
```
## （3）设置alpha参数
参数alpha控制被估计的系数的稀疏程度。sklearn提供了三种得到alpha参数的方法：
[LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)、[LassoLarsCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV)、[LassoLarsIC](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html#sklearn.linear_model.LassoLarsIC)

对于带有许多共线回归量的高维数据集，LassoCV效果更好。

[More Info——>Setting regularization parameter](http://scikit-learn.org/stable/modules/linear_model.html#setting-regularization-parameter)

## （4）多任务Lasso
[MultiTaskLasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html#sklearn.linear_model.MultiTaskLasso)是一个估计多任务回归问题的线性模型。这里`y`是一个2维数组，shape为`(n_samples, n_tasks)`。它的限制是对于多个回归任务，使用相同的特征。

[More Info——>Multi-task Lasso](http://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso)

# 5. Elastic Net
## （1）原理
[ElasticNet](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)是一个同时使用L1和L2作为正则化项的线性模型。它的目标函数是：

![](http://scikit-learn.org/stable/_images/math/51443eb62398fc5253e0a0d06a5695686e972d08.png)


更多理论参考：

[`https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf`](https://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&%20Hastie.pdf)

http://scikit-learn.org/stable/modules/linear_model.html#elastic-net
http://stats.stackexchange.com/a/184031
https://en.wikipedia.org/wiki/Elastic_net_regularization

# （2）设置参数
The class [ElasticNetCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV) can be used to set the parameters `alpha` ![](http://scikit-learn.org/stable/_images/math/877d234f4cec6974ce218fc2e975a486a7972dfd.png) and `l1_ratio`![](http://scikit-learn.org/stable/_images/math/9a51ab9a0b521705e1e8762fac6bdd6f11771758.png) by cross-validation.

# （3）多任务Elastic Net
http://scikit-learn.org/stable/modules/linear_model.html#multi-task-elastic-net

# 6. 逻辑回归
## （1）原理
[LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)

二分类L2正则目标函数：

![](http://scikit-learn.org/stable/_images/math/760c999ccbc78b72d2a91186ba55ce37f0d2cf37.png)

二分类L1正则化目标函数：

![](http://scikit-learn.org/stable/_images/math/6a0bcf21baaeb0c2b879ab74fe333c0aab0d6ae6.png)

[More Info——>`Logistic regression` User Guide](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)
[sklearn.linear_model.LogisticRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)

## （2）与最大熵模型关系

> 当类标签（class label）只有两个的时候，最大熵模型就是 logistic 回归模型。

参考[如何理解最大熵模型里面的特征?——知乎](https://www.zhihu.com/question/24094554/answer/108271031)

## （3）参数设置
[LogisticRegressionCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV) implements Logistic Regression with builtin cross-validation to find out the optimal C parameter. “newton-cg”, “sag” and “lbfgs” solvers are found to be faster for high-dimensional dense data, due to warm-starting. For the multiclass case, if multi_class option is set to “ovr”, an optimal C is obtained for each class and if the multi_class option is set to “multinomial”, an optimal C is obtained by minimizing the cross- entropy loss.

# 7. 多项式回归
## （1）原理
对于多项式回归模型，我们可以先扩展多项式特征，然后再使用上面的任何线性模型进行估计。

比如对于二阶多项式，我们有如下模型：

![](http://scikit-learn.org/stable/_images/math/1e1f74179df321954b823943c08d555a524e69f9.png)

原始特征为`[x1, x2]`,构造多项式特征后得到![](http://scikit-learn.org/stable/_images/math/2c0a9947255ef61995c3f5e6319fc7fdfa3503c6.png),此时再使用线性模型：

![](http://scikit-learn.org/stable/_images/math/618623438e33ecf053d364a0b938f056cd758b34.png)

sklearn提供了[PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures)来对线性特征进行多项式特征转换。
## （2）代码示例
```python
>>> from sklearn.preprocessing import PolynomialFeatures
>>> import numpy as np
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
       [2, 3],
       [4, 5]])
>>> poly = PolynomialFeatures(degree=2)
>>> poly.fit_transform(X)
array([[  1.,   0.,   1.,   0.,   0.,   1.],
       [  1.,   2.,   3.,   4.,   6.,   9.],
       [  1.,   4.,   5.,  16.,  20.,  25.]])
```
注意`interaction_only`参数设置

[More Info ——> Polynomial regression: extending linear models with basis functions](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)

# 8. 线性（二次）判别分析
## （1）原理
[LinearDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis)和[QuadraticDiscriminantAnalysis](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)都可以做分类器，它们做分类器的原理既可以通过贝叶斯决策理论来阐述，也可以通过非贝叶斯理论来阐述；LDA还可以做有监督的降维，降维的原理是设法将样本投影到一条直线上，使得同类样本的投影点尽可能接近、异类样本的投影点尽可能远离。

## （2）代码示例
*降维Example*:
```python
>>> import numpy as np
>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> lda = LinearDiscriminantAnalysis(n_components=1)
>>> lda.fit(X, y)
LinearDiscriminantAnalysis(n_components=1, priors=None, shrinkage=None,
              solver='svd', store_covariance=False, tol=0.0001)
>>> lda.transform(X)
array([[-1.73205081],
       [-1.73205081],
       [-3.46410162],
       [ 1.73205081],
       [ 1.73205081],
       [ 3.46410162]])
```
参数`n_components`控制输出的维度，它应当小于类标种类数（上例只有2种类标，我们将此参数设为了1）。可以从最终结果看到，输出的一维矩阵（列向量）中属于类别1的前三个样本，都被投影成负数，且距离接近；属于类别2的后三个样本都被投影成彼此接近的正数。

*分类Example*：
```python
>>> import numpy as np
>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = LinearDiscriminantAnalysis()
>>> clf.fit(X, y)
LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
              solver='svd', store_covariance=False, tol=0.0001)
>>> print(clf.predict([[-0.8, -1]]))
[1]
```

## （3）Shrinkage
当特征数大于样本数时，设置`shrinkage`参数，表明需要Shrinkage

[More Info ——> shrinkage](http://scikit-learn.org/stable/modules/lda_qda.html#shrinkage)

<br />

**Ref**

http://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models