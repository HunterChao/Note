# <p align='center'>第二章 模型评估与选择

> **概览**：本章先介绍了模型评估的一些基本概念，然后介绍了模型评估的方法——建立测试集，重点介绍了建立测试集的几种方法；对学习器泛化性能评估，不仅需要切实可行的实验估计方法，还需要有衡量模型泛化能力的指标，因而开始讨论性能度量；有了性能度量，我们并不直接取性能度量的值进行比较，基于某些原因，我们使用统计假设检验。

**本章内容**：

1. 经验误差与过拟合
2. 评估方法
3. 性能度量
4. 比较检验
5. 偏差与方差

### 1. 经验误差与过拟合

1）**了解几个名词的含义**：

- 错误率（error rate）
- 精读（accuracy）
- 训练误差（training error）/经验误差（empirical error）
- 泛化误差（generalization error）
- 过拟合（overfitting）、欠拟合（underfitting）
- 模型选择（model selection）
- 测试误差（testing error）

2）**过拟合和欠拟合的定义**

过拟合：当学习器把训练样本学得太好了的时候，很可能把训练样本自身的一些特点当做了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。这种现象在机器学习中成为“过拟合”。

欠拟合：指学习器对训练样本的一般性质尚未学好。

3）**过拟合无法彻底避免**

### 2. 评估方法

1）**我们使用测试误差作为泛化误差的近似**

2）**划分测试集和训练集时要注意：**
测试集应尽可能地与训练集互斥，即测试样本尽量不在训练集中出现、未在训练集中使用过。

3）**划分训练集和测试集的方法**有三种： 留出法（hold-out）、交叉验证法（cross validation）、自助法（bootstrapping）

4）**关于留出法**：
> **简介**：直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个集合作为测试集T。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。
>
> **注意**：
> 
> 1. 训练集和测试集的划分**要尽可能保持数据分布的一致性**，避免因数据划分过程引入额外的偏差而对最终结果产生影响。（例如**在分类任务中至少保持样本类别比例相似**）
> （对于这一点有必要解释一下：保留类别比例的采样方式也称作“分层采样stratified sampling”。比如我们的数据集中classA和classB的比例为2：1，我们在使用留出法进行采样时尽量也要保证训练集和测试集中的classA和classB的比例也为2：1）
>
> 2. 在给定训练集测试集样本比例后，仍存在多种划分方式，这些划分导致的模型评估结果也有差别。**一般采用若干次随机划分、重复进行试验评估后取平均值作为留出法的评估结果**。
> 3. 关于**训练集和测试集比例选择**：一般使用大约2/3~4/5的样本用于训练，其余样本作为测试。

5）**关于交叉验证法**：

> **简介**：交叉验证法先将数据集D划分为k个大小相近的互斥子集，每个子集Di尽可能保持数据分布一致性，即从D中通过分层采样得到。然后每次用k个子集的并集作为训练集，余下的那个子集作为测试集；这样可以获得k组训练/测试集，从而进行k次训练测试，最终返回k个测试结果的均值。
>
> **注意**:
> 
> 1. “交叉验证法”通常被称为“k折交叉验证法”，**k的常用取值为10**；其它的常用取值为5，20等。
> 2. 类似于留出法，将数据集D划分为k个子集同样存在多种划分方式，为减少因样本划分不同带来的差别，**k折交叉验证通常要随机使用不同的划分重复p次**，最终使用p次k折交叉验证结果的均值。常见的有“10次10折交叉验证”（训练/测试了100次）
>
>**留一法**：
>假设数据集中包含m个样本，令k=m，则得到了交叉验证法的一个特例：留一法（Leave-One-Out）

6）**关于自助法**
> **简介**：给定包含m个样本的数据集D，我们对它进行采样产生数据集D'：每次随即从D中挑选一个样本，将其拷贝放入D'，然后再将该样本放回初始数据集中，使此样本在下次仍可能被采样到；重复m次，我们得到了包含m个样本的数据集D'。初始数据集中有约36.8%的样本未出现在D'中。于是我们将D'作为训练集（m个），D-D'作为测试集。
>
>自助法的测试结果又称为“**包外估计**”（out-of-bag-estimate）
>
>**弊处**：自助法产生的数据集改变了初始数据集的分布，引入了估计偏差。

7）**三种方法的选择**
> **自助法**在数据集较小、难以有效划分训练/测试集时很有用。

>在初始数据量足够时，**留出法**和**交叉验证法**更常用。

8）**调参与最终模型**
> 机器学习通常涉及两类参数：**算法参数（超参数）**和**模型参数**。算法参数数量较少，通常在10以内；模型参数可能很多。

> **在模型选择完成后，学习算法和参数配置已选定，此时应使用数据集D重新训练模型**，进行预测。
> 
> 进行模型选择和调参时，我们需要使用**验证集**（validation set），它独立于训练集和测试集。


### 3. 性能度量

1）要评估学习器的性能，需要**把学习器的预测结果f(x)和真实标记y进行比较**。

2）对于**回归问题**，最常用的性能度量是“均方误差”（mean squared error）

表达式： ![](http://i.imgur.com/bpsIQwL.jpg)

更一般地，对于数据分布D和概率密度函数p(.),均方差可表示为
![](http://i.imgur.com/IjAmLdO.jpg)

<br>
 ***下面讨论的都是分类的性能度量***

3) **错误率和精度**
> **简介**： 最基本的两种性能度量。

>**错误率**：
>
>定义：
>![](http://i.imgur.com/vbhptvs.jpg)
>更一般的，对于数据分布D和概率密度函数p(.)
>![](http://i.imgur.com/dOjnWPt.jpg)

>**精度**:
>
>定义：
>![](http://i.imgur.com/B6oPeYP.jpg)
>更一般的，对于数据分布D和概率密度函数p(.)
>![](http://i.imgur.com/vX445AC.jpg)

4)**查准率、查全率和F1值**
> **二分类混淆矩阵**
> ![](http://i.imgur.com/inUAevI.jpg)
> 
> **准确率**（precision）**P = TP/(TP+FP)**
> 
> **召回率**（recall） **R = TP/(TP+FN)**

>**查准率和查全率往往相互矛盾**，一般查准率高时，查全率往往偏低。
>
><h4>关于P-R曲线</h4>
>![](http://i.imgur.com/mFnn8V3.jpg)
>绘制：根据学习器的预测结果对样例进行排序，排在最前面的是学习器认为“最可能”是正例的样本。按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查准率和查全率。以查准率为纵轴，以查全率为横轴作图，即可得到**查准率--查全率曲线**，简称P-R曲线。
>
>