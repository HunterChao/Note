文本分类：

1. 预处理
2. 特征选择
    - DF (Document Frequency)
    - 信息增益 (Information Gain, IG)
    - 熵 (Entropy)
    - 相对熵 (Relative Entropy)
    - χ² 统计量 (Chi-Square)
    - 互信息 (Mutual Information)
    - Robertson & Sparck Jones公式
    - 发生比 (Odds)
    - Term Strength
    - 性能比较
3. 计算权重
    - tf-idf

# 1. 预处理
这一步包含以下几个内容：

- **去除标签**：这种情况适用于你拿到的数据是带有html标签的时候。推荐使用Python的`BeautifulSoup`包。
- **数据清洗**：去掉不合适的噪声文档或文档内垃圾数据.
- **分词，词性标注、过滤**：一般来说文本分类使用的是**词特征**，所以我们需要对文章进行分词；
为什么要标注词性呢？因为往往具有类别表征能力的词特征往往是名词、动词等(并不绝对)。中文分词工具
广泛使用的是[jieba分词](https://github.com/fxsjy/jieba)。
- **停用词去除**：停用词，比如`‘的’`、`‘了’`，是一定需要去除的，因为它不具有任何类别表征能力。
- **词频统计**：词频统计是我们后面进行**特征提取**、**特征权值计算**的基础。

**实践经验**：

*词频统计时，你需要统计两种词频：`语料库词频` 和 `文档词频`，推荐使用的数据结构是python中的`dict`:*
```
# 数据存储示例
corpus_freq_dict = {
    word1: freq1,
    word2: freq2,
    ....
}

doc_freq_dict = {
    doc1: {
        word1: freq1,
        word2: freq2,
        ...
    },
    doc2: {
        word1: freq1,
        word2: freq2,
        ...
    },
    ...
}
```

# 2. 特征选择
用词做特征，不做特征选择这一步的话，很容易出现上万维、甚至几十万维，
这么多的维度对计算来说可能就是个灾难。即使你的计算资源足够应付，
那也是对资源的浪费，因为真正对分类起作用的词，可能就只是一少部分。

一般将维度选择在500~2000维，当然也不绝对。

特征选择有很多方式，包括：

- **DF** (Document Frequency)
- **信息增益** (Information Gain, IG)
- **熵** (Entropy)
- **相对熵** (Relative Entropy)
- **χ² 统计量** (Chi-Square)
- **互信息** (Mutual Information)
- **Robertson & Sparck Jones公式**
- **发生比** (Odds)
- **Term Strength**

我们一一介绍：

## （1）DF (Document Frequency)
DF即**文档频率**：所有文档集合中出现特征term的文档数目。

- Term的DF小于某个阈值去掉(太少，没有代表性)
- Term的DF大于某个阈值也去掉(太多，没有区分度)

这种方法很简单，效果也不错，当你没有足够的计算资源时，你能使用它用很低的成本获得较高的准确度。


## （2）信息增益（Information Gain）
回忆一下**熵**的定义：

假设有一个变量X，它可能的取值有`n`多种，分别是`x1`，`x2`，……，`xn`，
每一种取到的概率分别是`P1`，`P2`，……，`Pn`，那么`X`的熵就定义为：

![熵](http://i.imgur.com/1DEkpNN.png)

<br />

再回忆一下**条件熵**的定义：

![条件熵](http://i.imgur.com/SlzYc6b.png)

<br />

信息增益指的是熵的变化。这里我们需要计算的是一个term为整个分类所能提供的信息增益。
(即不考虑任何特征的熵和考虑该特征后的熵的差值)。公式如下：

![信息增益](http://i.imgur.com/Z7qSouJ.png)

注意：考虑特征term后的熵用条件熵表示。

**条件熵越小，则信息增益越大，则特征越重要**。这很容易理解，
因为条件熵越小，说明了在此特征下，类别的不确定性越低。

## （3）熵 (Entropy)
公式如下：

![熵](http://i.imgur.com/AbvGKm9.png)

你可以看到它其实是条件熵的一部分。

- 熵越大，说明分布越均匀，越有可能出现在多个类别中。
- 熵越小，说明分布越倾斜，越有可能出现在单个类别中。

所以熵越小，特征越重要。

## （4）相对熵 (Relative Entropy)
回忆一下**相对熵定义**：

相对熵（relative entropy）又称为**KL散度**（Kullback–Leibler divergence，简称KLD）。
它度量两个概率分布`P`和`Q`差别的非对称性。(离散随机变量)公式：

![相对熵](http://i.imgur.com/WIfvah9.png)

<br />

这里我们需要度量的是**文本类别的概率分布**和**在出现了某个特定词汇条件下的文本类别的概率分布**之间的距离。
因此，我们此时的公式为：

![相对熵](http://i.imgur.com/VvNJOU5.png)

相对熵越大，特征词对文本类别分布的影响也越大，特征就越重要。

## （5）χ² 统计量 (Chi-Square)
我们先说**结论**：

我们使用卡方统计度量**term**和**类别**独立性的缺乏程度，卡方越大，独立性越小，
相关性越大，特征越重要。公式：

![卡方统计公式](http://i.imgur.com/7M8w7RV.png)

上式其实只是检验了term `t`与类别`c`的相关性,在**多分类**下我们需检验`t`与所有类别的相关性，然后取加权平均(权值为类的概率)，公式：

![多分类卡方计算](http://i.imgur.com/9gxPogN.png)

**注意**：`Max(Chi-square)`的使用也很常见,它不过是取多个类别计算出的卡方的最大值罢了。
<br />

**推导：**

卡方检验最基本的思想就是**通过观察实际值与理论值的偏差来确定理论的正确与否**。具体做的时候常常先假设两个变量确实是独立的（“原假设”），然后观察实际值（观察值）与理论值（这个理论值是指“如果两者确实独立”的情况下应该有的值）的偏差程度。如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。

理论值为`E`，实际值为`x`，**偏差程度的计算公式**为：

![卡方偏差](http://i.imgur.com/dasMQ83.png)

这个式子就是卡方检验使用的**差值衡量公式**。当提供了数个样本的观察值`x1`，`x2`，`……`，`xi`，`……`,`xn`之后，代入到式中就可以求得卡方值，用这个值与事先设定的阈值比较，如果大于阈值（即偏差很大），就认为原假设不成立，反之则认为原假设成立。

在**文本分类的特征选择阶段**，一般使用`“词t与类别c不相关”`来做原假设，计算出的卡方值越大，说明对原假设的偏离越大，我们越倾向于认为原假设的反面情况是正确的。

对照下表：

![table](http://i.imgur.com/RH1OgUw.png)

根据原假设，**类别`c`中包含`t`的文档比例**应与**所有文档中包含`t`的文档比例**相同。故，`A`的理论值`A'`应为：

![A'](http://i.imgur.com/95pXMJW.png)

所以`A'`与`A`的差值根据差值公式有：

![A-A'](http://i.imgur.com/U6Z2CJX.png)

同理，我们可以算出`D(B, B')`, `D(C, C')`, `D(D, D')`。

所以，卡方等于：

![最终卡方](http://i.imgur.com/XunqxrT.png)

注意：这里`N=(A+B+C+D)`。我们的目的是比较所有term的卡方值大小，你会发现`(A+C)`,`(B+D)`,`N`在所有term的卡方计算中都是一样的，所以可以去掉这几项，所以你会得到我们结论中的第一个公式。

你可能会有疑问，为什么对于二分类，只需检验与类`c`的独立性，不考虑与类`~c`的独立性。你可以自己推导下`Chi(term,~c)`的公式，它与`Chi(term,c)`的公式一模一样。这也很符合常理，只有的两类，你检验`c`,就是检验了`~c`。

## （6）互信息 (Mutual Information)
互信息被**定义**为：

![MI](http://i.imgur.com/U8Q4eM3.png)

它度量联合概率`P(AB)`和边缘概率之积`P(A)P(B)`的距离。这也很容易理解，当`A`、`B`相互独立时，`P(AB)`和`P(A)P(B)`是相等的,此时互信息最小，为`0`。

<br />

在**文本分类**中，我们度量`P(t^c)`与`P(t)P(c)`的距离。公式如下：

![互信息](http://i.imgur.com/YpNGY5a.png)

互信息越大，距离越小，term与重要。实践场景中下互信息的结果是比较差的。

## （7）Robertson & Sparck Jones公式

![RSJ](http://i.imgur.com/ZZq2INK.png)

## （8）发生比 (Odds Ratio)
**定义**：

**Odds Ratio** compares the odds of a feature occurring in one category with the odds for it occurring in another category. It gives a positive score to features that occur more often in one category than in the other, and a negative score if it occurs more in the other. A score of zero means the the odds for a feature to occur in one category is exactly the same as the odds for it to occur in the other, since `ln (1) = 0`.

公式：

![OR](http://i.imgur.com/ZDXHTeA.png)

（公式并没看懂0.0）

发生比绝对值越大，说明term受类分布影响越大，特征越重要。

## （9）Term Strength
**Term Strength** is a technique for Feature Selection in Text Mining

- it doesn't need a pre-defined list of **Stop Words** - it discovers them automatically.
- so it's a technique for vocabulary reduction in text retrieval.
- this method estimates term importance based on how often a term appears in "related" documents.

**Strength of a term `t`**

- measures how informative a word is for identifying two related documents.
- `s(t)=P(t∈y∣t∈x)`
- for two related documents `x`,`y` what's the probability that `t` belongs to `y` given it belongs to `x`?
- estimate `s(t)` on training data using Maximum Likelihood Estimation.(此处存疑惑)

**What does it mean "related"?**

- if we know the labels of these documents, then related are those that belong to the same category.
- what about Unsupervised Learning?
    - use Cosine Similarity to find most related documents.
    - set some threshold `t` and let all pairs with cosine `>t` be related.

## （10）性能比较
![KNN](http://i.imgur.com/U8UN4Rt.png)

![LLSF](http://i.imgur.com/AHwzhxT.png)

注意：LLSF(Linear Least Square Fit)是**最小二乘拟合**的简写。

我们可以看到在特征超过2000维时，`CHI`、`IG`、`DF`的效果都是很好的，其中`CHI`的效果最好。`TS`、`MI`的效果比较差，其中`MI`的效果最差。

# 3. 计算权重
## （1）tf-idf
**简介**：

TF-IDF（term frequency–inverse document frequency）是一种用于资讯检索与文本挖掘的常用**加权**技术。TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降**。

<br />

**原理及公式**：

TF-IDF的**主要思想**是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上是：`TF * IDF`。
- `TF`是**词频**（Term Frequency），它表示词条在文档`d`中出现的频率。
- `IDF`是**逆向文件频率**（Inverse Document Frequency）。IDF的主要思想是：如果包含词条`t`的文档越少，`IDF`越大，则说明词条`t`具有很好的类别区分能力。

**公式**：

![TFIDF](http://i.imgur.com/q07Nn76.png)

注意：`idf`分母文件数加`1`是为了防止出现分母为`0`发生。

<br />

**Ref**

[特征选择方法之信息增益](http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html)

[条件熵 wikipedia](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E7%86%B5)

[相对熵 wikipedia](https://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5)

[文本分类特征选择方法——卡方检验信息增益](http://blog.sina.com.cn/s/blog_6622f5c30101datu.html)

[特征选择之互信息](http://blog.csdn.net/oanqoanq/article/details/9238817)

[互信息_wikipedia](https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF)

[Term Strength - ML Wiki](http://mlwiki.org/index.php/Term_Strength)

[A comparative study on feature selection in text categorization](http://courses.ischool.berkeley.edu/i256/f06/papers/yang97comparative.pdf)

[TF-IDF_wikipedia](https://zh.wikipedia.org/wiki/TF-IDF)
