# 1. 预处理
这一步包含以下几个内容：

- **去除标签**：这种情况适用于你拿到的数据是带有html标签的时候。推荐使用Python的`BeautifulSoup`包。
- **数据清洗**：去掉不合适的噪声文档或文档内垃圾数据.
- **分词，词性标注、过滤**：一般来说文本分类使用的是**词特征**，所以我们需要对文章进行分词；
为什么要标注词性呢？因为往往具有类别表征能力的词特征往往是名词、动词等(并不绝对)。中文分词工具
广泛使用的是[jieba分词](https://github.com/fxsjy/jieba)。
- **停用词去除**：停用词，比如`‘的’`、`‘了’`，是一定需要去除的，因为它不具有任何类别表征能力。
- **词频统计**：词频统计是我们后面进行**特征提取**、**特征权值计算**的基础。

**实践经验**：

*词频统计时，你需要统计两种词频：`语料库词频` 和 `文档词频`，推荐使用的数据结构是python中的`dict`:*
```
# 数据存储示例
corpus_freq_dict = {
    word1: freq1,
    word2: freq2,
    ....
}

doc_freq_dict = {
    doc1: {
        word1: freq1,
        word2: freq2,
        ...
    },
    doc2: {
        word1: freq1,
        word2: freq2,
        ...
    },
    ...
}
```

# 2. 特征选择
用词做特征，不做特征选择这一步的话，很容易出现上万维、甚至几十万维，
这么多的维度对计算来说可能就是个灾难。即使你的计算资源足够应付，
那也是对资源的浪费，因为真正对分类起作用的词，可能就只是一少部分。

一般将维度选择在500~2000维，当然也不绝对。

特征选择有很多方式，包括：

- **DF** (Document Frequency)
- **信息增益** (Information Gain, IG)
- **熵** (Entropy)
- **相对熵** (Relative Entropy)
- **χ² 统计量** (Chi-Square)
- **互信息** (Mutual Information)
- **Robertson & Sparck Jones公式**
- **发生比** (Odds)
- **Term Strength**

我们一一介绍：

## （1）DF (Document Frequency)
DF即**文档频率**：所有文档集合中出现特征term的文档数目。

- Term的DF小于某个阈值去掉(太少，没有代表性)
- Term的DF大于某个阈值也去掉(太多，没有区分度)

这种方法很简单，也很好操作。


## （2）信息增益（Information Gain）
回忆一下**熵**的定义：

假设有一个变量X，它可能的取值有`n`多种，分别是`x1`，`x2`，……，`xn`，
每一种取到的概率分别是`P1`，`P2`，……，`Pn`，那么`X`的熵就定义为：

![熵](http://i.imgur.com/1DEkpNN.png)

<br />

再回忆一下**条件熵**的定义：

![条件熵](http://i.imgur.com/SlzYc6b.png)

<br />

信息增益指的是熵的变化。这里我们需要计算的是一个term为整个分类所能提供的信息增益。
(即不考虑任何特征的熵和考虑该特征后的熵的差值)。公式如下：

![信息增益](http://i.imgur.com/Z7qSouJ.png)

注意：考虑特征term后的熵用条件熵表示。

**条件熵越小，则信息增益越大，则特征越重要**。这很容易理解，
因为条件熵越小，说明了在此特征下，类别的不确定性越低。

## （3）熵 (Entropy)
公式如下：

![熵](http://i.imgur.com/AbvGKm9.png)

你可以看到它其实是条件熵的一部分。

- 熵越大，说明分布越均匀，越有可能出现在多个类别中。
- 熵越小，说明分布越倾斜，越有可能出现在单个类别中。

所以熵越小，特征越重要。

## （4）相对熵 (Relative Entropy)
回忆一下**相对熵定义**：

相对熵（relative entropy）又称为**KL散度**（Kullback–Leibler divergence，简称KLD）。
它度量两个概率分布`P`和`Q`差别的非对称性。(离散随机变量)公式：

![相对熵](http://i.imgur.com/WIfvah9.png)

<br />

这里我们需要度量的是**文本类别的概率分布**和**在出现了某个特定词汇条件下的文本类别的概率分布**之间的距离。
因此，我们此时的公式为：

![相对熵](http://i.imgur.com/VvNJOU5.png)

相对熵越大，特征词对文本类别分布的影响也越大，特征就越重要。

## （5）χ² 统计量 (Chi-Square)
我们先说**结论**：

我们使用卡方统计度量**term**和**类别**独立性的缺乏程度，卡方越大，独立性越小，
相关性越大，特征越重要。公式：

![卡方统计公式](http://i.imgur.com/7M8w7RV.png)

<br />

**推导：**

卡方检验最基本的思想就是**通过观察实际值与理论值的偏差来确定理论的正确与否**。具体做的时候常常先假设两个变量确实是独立的（“原假设”），然后观察实际值（观察值）与理论值（这个理论值是指“如果两者确实独立”的情况下应该有的值）的偏差程度。如果偏差足够小，我们就认为误差是很自然的样本误差，是测量手段不够精确导致或者偶然发生的，两者确确实实是独立的，此时就接受原假设；如果偏差大到一定程度，使得这样的误差不太可能是偶然产生或者测量不精确所致，我们就认为两者实际上是相关的，即否定原假设，而接受备择假设。

理论值为E，实际值为x，偏差程度的计算公式为：


















<br />

**Ref**

[特征选择方法之信息增益](http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html)

[条件熵 wikipedia](https://zh.wikipedia.org/wiki/%E6%9D%A1%E4%BB%B6%E7%86%B5)

[相对熵 wikipedia](https://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5)