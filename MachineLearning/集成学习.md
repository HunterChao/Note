目录
1. 基本原理
    - Boosting
    - Bagging
    - 随机森林
2. 实践

# 1. 基本原理
集成学习（Ensemble Learning）由多个学习器来完成学习任务。它大致可分为两大类：
1. 基学习器之间存在强依赖关系，必须串行生成。
2. 基学习器之间不存在强依赖关系，可并行生成。

以前者为代表的是Boosting，以后者为代表的是Bagging和随机森林。

# （1）Boosting
这类算法的**工作原理**类似：先从初始训练集训练一个基学习器，再根据基学习器的表现对训练样本分布进行重新调整，使得先前基学习器做错的样本在后续受到持续关注，基于调整后的样本分部来训练下一个基学习器，如此重复，直到基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

从偏差——方差分解的角度来看，Boosting主要关注的是降低偏差。


# （2）Bagging
**基本原理**：个体学习器要好而不同。基学习器要有一定的准确性，同时也应该有足够的多样性。

# （3）随机森林
随机森林是Bagging的一个扩展变体。它的基学习器的"好"体现在**使用CART树作为基学习器**，它的多样性，即"随机性"体现在两个方面：

1. 随机属性选择（每棵树都会从原始属性集中随机选择k个属性作为自己的属性集）
2. 样本随机（每棵树都会从原始样本集中进行自主采样，并将其作为自己的训练样本集）

