目录
1. 理论
    - 基本原理
    - 属性划分选择
        - 信息增益
        - 基尼指数
    - 树的生成
        - 步骤
        - 停止条件
    - 树剪枝
        - 预剪枝
        - 后剪枝
    - 其它算法
        - C4.5
        - CART
2. 实践

# 1. 理论
## （1）基本原理
决策树呈树形结构，叶节点对应于决策结果，其它节点对应于属性测试。可以把它看成一个if-then规则的集合，也可以看成给定特征条件下，类的条件概率分布。（一个决策树将特征空间划分成一个又一个的小矩形，每个小矩形对应着决策树中相应的一个带类标的叶节点）。

决策树学习包括三个过程：**属性划分选择**、**树的生成**、**树剪枝**。

## （2）属性划分选择
### 信息增益
信息增益被定义为：经验熵与经验条件熵之差。更严谨地表述：

![](http://img2016.itdadao.com/d/file/tech/2017/02/22/it3225892214402410.png)
![](http://img2016.itdadao.com/d/file/tech/2017/02/22/it3225892214402411.png)
上面`k`表示类别不同取值，`i`表示属性A的不同取值。
### 基尼指数
基尼指数反映数据集D的纯度，含义是：从数据集D中随机取两个样本，其类标不一致的概率。更严谨地表述：

![Imgur](http://i.imgur.com/vZiLzF0.png)

## （3）树的生成
### 步骤
1. 生成一个节点
2. 选择最优属性
3. 根据所选属性的不同取值分裂生成新的节点，递归下去。
### 递归终止条件

# 2. 实践
