目录
1. 理论
    - 基本原理
    - 属性划分选择
        - 信息增益
        - 基尼指数
    - 树的生成
        - 步骤
        - 停止条件
    - 树剪枝
        - 预剪枝
        - 后剪枝
    - 其它算法
        - C4.5
        - CART
    - 决策树优缺点
        - 优点
        - 缺点
2. 实践
    - 分类
    - 回归
    - 多任务学习
    - Tips

# 1. 理论
## （1）基本原理
决策树呈树形结构，叶节点对应于决策结果，其它节点对应于属性测试。可以把它看成一个if-then规则的集合，也可以看成给定特征条件下，类的条件概率分布。（一个决策树将特征空间划分成一个又一个的小矩形，每个小矩形对应着决策树中相应的一个带类标的叶节点）。

决策树学习包括三个过程：**属性划分选择**、**树的生成**、**树剪枝**。

## （2）属性划分选择
### 信息增益
信息增益被定义为：经验熵与经验条件熵之差。更严谨地表述：

![](http://img2016.itdadao.com/d/file/tech/2017/02/22/it3225892214402410.png)
![](http://img2016.itdadao.com/d/file/tech/2017/02/22/it3225892214402411.png)

上面`k`表示类别不同取值，`i`表示属性A的不同取值。
### 基尼指数
基尼指数反映数据集D的不纯度，含义是：从数据集D中随机取两个样本，其类标不一致的概率。更严谨地表述：

![Imgur](http://i.imgur.com/vZiLzF0.png)

## （3）树的生成
### 步骤
1. 生成一个节点
2. 选择最优属性
3. 根据所选属性的不同取值分裂生成新的节点，递归下去。
### 递归终止条件
有三种情形会导致递归返回：
1. 当前节点包含的样本全部属于同一类别
2. 当前属性集为空
3. 当前节点包含的样本集合为空，不能划分

## （4）树剪枝
通过剪枝：降低过拟合风险。

决定是否剪枝使用**验证集**进行评估。
### 预剪枝
预剪枝是指在决策树生成过程中，对每个节点在划分前进行估计，如果划分不能带来决策树泛化性能提升，则停止划分。
### 后剪枝
后剪枝是指先从训练集生成一棵完整的决策树，然后自底向上对非叶节点进行考察，如果将该节点对应的子树替换为叶节点能带来决策树泛化性能的提升，则将该子树替换为叶节点。

## （5）其它算法
### C4.5
C4.5是在ID3的基础上进行改进的。在最优属性选择时，ID3使用信息增益，C4.5使用信息增益比。因为信息增益往往对可取值数目较多的属性有所偏好。
### CART
CART全称Classification and Regression Tree，同样由特征选择、树的生成、剪枝组成，即可以用来分类，也可以用于回归。

## （6）决策树优缺点
### 优点
- 容易理解和解释，树可以被可视化。
- 只需很少的数据预处理工作。比如，不需要归一化。
- 使用决策树模型预测时，成本是对数阶的。
- 能够处理数值类型和离散类型的属性。
- 能够处理多输出问题。
- 使用白盒模型（相对于神经网络这种黑盒模型来说）。
- 能够使用统计检验验证模型。
- 尽管训练的模型与真实模型有点违背，也可能运行地很好。
### 缺点
- 容易过拟合。这个问题可以通过剪枝、设置叶节点最少样本数、控制树的最大深度等措施解决。
- 不稳定。小的数据变更可能会导致一个完全不同的决策树被生成，这个问题可以通过集成学习环节。
- 学习最优决策书是NP完全问题。我们一般采用启发式贪心算法对每个节点进行最优决策，但并不能保证获得全局最优。这个问题可以被缓解，通过在集成学习器中训练多棵决策树。
- 有一些概念很难学习。因为决策树并不能很容易将它们表达出来，比如异或问题。
- 当类分布不平衡时，决策树学习器容易创建有偏的树。在训练之前建议先平衡数据集。
# 2. 实践
## （1）分类
使用[DecisionTreeClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)接口。
*Code Example*:
```python
>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)
# 预测
>>> clf.predict([[2., 2.]])
array([1])
# 带概率的预测
>>> clf.predict_proba([[2., 2.]])
array([[ 0.,  1.]])
```

生成的决策树模型可以被导出以及展示，这里不解释。详细见文档[sklearn——DecisionTree](http://scikit-learn.org/stable/modules/tree.html#classification)

## （2）回归
使用[DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)接口。
*Code Example*:
```python
>>> from sklearn import tree
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = tree.DecisionTreeRegressor()
>>> clf = clf.fit(X, y)
>>> clf.predict([[1, 1]])
array([ 0.5])
```

## （3）多任务学习
所谓多任务学习，
- 对于分类来讲：一个样本对应多个类标；
- 对于回归来讲：一个样本对应多个目标数值。

多任务学习的使用方式和普通决策树使用方式一样，只不过y不再是一个向量，而是一个`n_samples*m_tasks`维的矩阵。
## （4）Tips
这里是一些使用决策树的小窍门：

1. 决策树在特征很多时容易过拟合。所以在使用前你需要计算一下数据的**样本特征比**：`样本数/特征数`。
2. 考虑在训练前对数据降维（PCA，ICA，特征选择）。
3. 当你在训练时，可视化你的树通过`export`函数。建议使用`max_depth=3`作为初始树深度，以便于你能容易看出你的模型是如何fit数据的，然后增加树的深度。
4. 使用`max_depth`参数控制树的大小，防止过拟合。
5. 使用`min_samples_split`或`min_samples_leaf`去控制所有节点或叶节点大小。尝试使用`5`作为初始值。这两个参数控制一个就好了，`min_samples_split`在文献中更常见。
6. 当数据集有明显的类别不平衡分布时，首先平衡数据集。
7. 如果特征矩阵非常稀疏的话，建议首先转化成csc稀疏矩阵表示方式，这样训练起来要快几个数量级。

<br >

**Ref**

http://scikit-learn.org/stable/modules/tree.html#decision-trees