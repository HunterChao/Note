目录：

1. 监督学习
2. 统计学习三要素
    - 模型
    - 策略
    - 算法


# 1. 监督学习

有两点需要**注意**：

1. 监督学习的基本**假设**：随机变量`X`和`Y`具有联合概率分布`P(X,Y)`。
2. 监督学习模型可以是**概率模型**或者**非概率模型**，由条件概率分布`P(Y|X)`或者决策函数`Y=ƒ(X)`表示。预测时，写作：`P(y|x)`或`y=ƒ(x)`。

# 2. 统计学习三要素

统计学习方法都是由模型、策略和算法构成。可以表示为：
```
学习方法 = 模型 + 策略 + 算法
```
## （1）模型
在监督学习过程中，**模型就是所要学习的条件概率分布或决策函数**。模型的假设空间包含所有可能的条件概率分布或决策函数。

假设空间用`Ϝ`表示。
- 假设空间可以定义为**决策函数的集合**：`Ϝ={ƒ|Y=ƒ(X)}`。其中`X`和`Y`分别是定义在输入空间和输出空间上的变量。此时`Ϝ`通常是一个由参数向量决定的**函数簇**：`Ϝ={ƒ|Y=ƒ(X; θ),θ∈R^n}`。向量参数`θ`取值于`n`维欧式空间`R^n`,称为参数空间。
- 假设空间也可以定义为**条件概率的集合**：`Ϝ={P|P(Y|X)}`。其中，`X`和`Y`分别是定义在输入空间和输出空间上的变量。此时`Ϝ`通常是一个由参数向量决定的**条件概率分布簇**：`Ϝ={P|P(Y|X; θ),θ∈R^n}`。向量参数`θ`取值于`n`维欧式空间`R^n`,它也是参数空间。

## （2）策略

策略解决的是**如果选择最优模型**的问题。我们常常有两种策略可以选择：**经验风险最小化策略**和**结构风险最小化策略**。我们先从损失函数谈起：

<br />

**损失函数**

监督学习问题是在假设空间`Ϝ`中选取模型`ƒ`作为决策函数，对于给定的输入`X`，由`ƒ(X)`给出相应的输出`Y'`,预测值`Y'`和真实值`Y`可能一致，也可能不一致，我们用**损失函数**(loss function)或**代价函数**(cost function)来度量预测错误程度。损失函数是`ƒ(X)`和`Y`的非负实值函数，记作`L(Y,ƒ(X))`。

常见的损失函数：
- 0-1损失函数(0-1 loss function)

    ![01lf](http://i.imgur.com/fD0KDOw.png)

- 平方损失函数(quadratic loss function)

    ![qlf](http://i.imgur.com/QkcO76Q.png)

- 绝对损失函数(absolute loss function)

    ![alf](http://i.imgur.com/sDhGZhm.png)

- 对数损失函数(logarithmic loss function)或者对数似然损失函数(log-likelihood loss function)

    ![llf](http://i.imgur.com/laGHvHH.png)

损失函数越小，模型就越好。

<br />

**风险函数**

风险函数(risk function)又称**期望损失**(expected loss),被定义为：

![risk function](http://i.imgur.com/T1BKIHo.png)

含义为：模型`ƒ(X)`关于联合概率分布`P(X,Y)`的平均意义下的损失。

<br />

**经验风险**

期望风险![期望风险](http://i.imgur.com/TcJ7rmv.png)是模型关于联合分布的期望损失，经验风险![经验风险](http://i.imgur.com/txu0oVE.png)是模型关于训练样本集的平均损失。经验风险被定义为：

![经验风险公式](http://i.imgur.com/qkGzSAG.png)

根据大数定理，当样本容量`N`趋于无穷时，经验风险趋于期望风险。所以我们常常用经验风险估计期望风险，但是由于训练样本有限，其效果常常并不理想，可以对经验风险进行一定的矫正。

<br />

**经验风险最小化和结构风险最小化**

- **经验风险最小化**(Expirical Risk Minimization, ERM)的策略认为经验风险最小的模型为最优模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：

    ![经验风险最小化](http://i.imgur.com/tjaoO87.png)

    其中`ϝ`是假设空间。当样本容量足够大时，经验风险最小化能保证有很好的学习效果；但是当样本容量很小时，经验风险最小化学习的效果就不是很好了，容易产生过拟合问题。

    **极大似然估计**(Maximum likelihood estimation)是经验风险最小化的一个例子。

- **结构风险最小化**(Structural Risk Minimization， SRM)是为了防止过拟合而提出来的策略。它等价于正则化(Regularization)。结构风险在经验风险基础上加上模型复杂度的正则化项(Regularizer)或罚项(Penalty Term)。在假设空间、损失函数以及训练数据集确定的情况下，结构风险最小化定义为：

    ![结构风险最小化](http://i.imgur.com/bWLZrBA.png)

    其中`J(ƒ)`为模型的复杂度，是定义在假设空间`ϝ`上的泛函。模型`ƒ`越复杂，复杂度`J(ƒ)`就越大；否则，复杂度越小。`λ>=0`是系数，或者说权重，用来权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。它训练出的模型往往对训练数据以及测试数据都有较好的预测。

    贝叶斯估计中的最大后验概率估计(Maximum Posterior Probability Estimation, MAP)是一个结构风险最小化的例子。

**注意**：

这里我们已经将监督学习问题转化为了经验风险或者结构风险函数的最优化问题。此时经验风险函数或结构风险函数是最优化的目标函数。

## （3）算法

上面我们已经将监督学习问题转化为优化问题，那么对于统计学习来讲，“算法”就指的是求解最优化问题的算法了。

如果最优化问题有显式的解析解，这个最优化问题就比较简单；但通常解析解不存在，这就需要数值计算的方法求解。

