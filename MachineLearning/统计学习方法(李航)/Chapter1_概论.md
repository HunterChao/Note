目录：

1. 监督学习
2. 统计学习三要素
    - 模型
    - 策略
    - 算法
3. 模型评估与模型选择
    - 训练误差与测试误差
    - 过拟合与模型选择
4. 正则化
5. 泛化能力
    - 泛化误差
    - 泛化误差上界
6. 生成模型和判别模型
7. 分类问题
8. 标注问题
9. 回归问题

# 1. 监督学习

有两点需要**注意**：

1. 监督学习的基本**假设**：随机变量`X`和`Y`具有联合概率分布`P(X,Y)`。
2. 监督学习模型可以是**概率模型**或者**非概率模型**，由条件概率分布`P(Y|X)`或者决策函数`Y=ƒ(X)`表示。预测时，写作：`P(y|x)`或`y=ƒ(x)`。

# 2. 统计学习三要素

统计学习方法都是由模型、策略和算法构成。可以表示为：
```
学习方法 = 模型 + 策略 + 算法
```
## （1）模型
在监督学习过程中，**模型就是所要学习的条件概率分布或决策函数**。模型的假设空间包含所有可能的条件概率分布或决策函数。

假设空间用`Ϝ`表示。
- 假设空间可以定义为**决策函数的集合**：`Ϝ={ƒ|Y=ƒ(X)}`。其中`X`和`Y`分别是定义在输入空间和输出空间上的变量。此时`Ϝ`通常是一个由参数向量决定的**函数簇**：`Ϝ={ƒ|Y=ƒ(X; θ),θ∈R^n}`。向量参数`θ`取值于`n`维欧式空间`R^n`,称为参数空间。
- 假设空间也可以定义为**条件概率的集合**：`Ϝ={P|P(Y|X)}`。其中，`X`和`Y`分别是定义在输入空间和输出空间上的变量。此时`Ϝ`通常是一个由参数向量决定的**条件概率分布簇**：`Ϝ={P|P(Y|X; θ),θ∈R^n}`。向量参数`θ`取值于`n`维欧式空间`R^n`,它也是参数空间。

## （2）策略

策略解决的是**如果选择最优模型**的问题。我们常常有两种策略可以选择：**经验风险最小化策略**和**结构风险最小化策略**。我们先从损失函数谈起：

<br />

**损失函数**

监督学习问题是在假设空间`Ϝ`中选取模型`ƒ`作为决策函数，对于给定的输入`X`，由`ƒ(X)`给出相应的输出`Y'`,预测值`Y'`和真实值`Y`可能一致，也可能不一致，我们用**损失函数**(loss function)或**代价函数**(cost function)来度量预测错误程度。损失函数是`ƒ(X)`和`Y`的非负实值函数，记作`L(Y,ƒ(X))`。

常见的损失函数：
- 0-1损失函数(0-1 loss function)

    ![01lf](http://i.imgur.com/fD0KDOw.png)

- 平方损失函数(quadratic loss function)

    ![qlf](http://i.imgur.com/QkcO76Q.png)

- 绝对损失函数(absolute loss function)

    ![alf](http://i.imgur.com/sDhGZhm.png)

- 对数损失函数(logarithmic loss function)或者对数似然损失函数(log-likelihood loss function)

    ![llf](http://i.imgur.com/laGHvHH.png)

损失函数越小，模型就越好。

<br />

**风险函数**

风险函数(risk function)又称**期望损失**(expected loss),被定义为：

![risk function](http://i.imgur.com/T1BKIHo.png)

含义为：模型`ƒ(X)`关于联合概率分布`P(X,Y)`的平均意义下的损失。

<br />

**经验风险**

期望风险![期望风险](http://i.imgur.com/TcJ7rmv.png)是模型关于联合分布的期望损失，经验风险![经验风险](http://i.imgur.com/txu0oVE.png)是模型关于训练样本集的平均损失。经验风险被定义为：

![经验风险公式](http://i.imgur.com/qkGzSAG.png)

根据大数定理，当样本容量`N`趋于无穷时，经验风险趋于期望风险。所以我们常常用经验风险估计期望风险，但是由于训练样本有限，其效果常常并不理想，可以对经验风险进行一定的矫正。

<br />

**经验风险最小化和结构风险最小化**

- **经验风险最小化**(Expirical Risk Minimization, ERM)的策略认为经验风险最小的模型为最优模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：

    ![经验风险最小化](http://i.imgur.com/tjaoO87.png)

    其中`ϝ`是假设空间。当样本容量足够大时，经验风险最小化能保证有很好的学习效果；但是当样本容量很小时，经验风险最小化学习的效果就不是很好了，容易产生过拟合问题。

    **极大似然估计**(Maximum likelihood estimation)是经验风险最小化的一个例子。

- **结构风险最小化**(Structural Risk Minimization， SRM)是为了防止过拟合而提出来的策略。它等价于正则化(Regularization)。结构风险在经验风险基础上加上模型复杂度的正则化项(Regularizer)或罚项(Penalty Term)。在假设空间、损失函数以及训练数据集确定的情况下，结构风险最小化定义为：

    ![结构风险最小化](http://i.imgur.com/bWLZrBA.png)

    其中`J(ƒ)`为模型的复杂度，是定义在假设空间`ϝ`上的泛函。模型`ƒ`越复杂，复杂度`J(ƒ)`就越大；否则，复杂度越小。`λ>=0`是系数，或者说权重，用来权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。它训练出的模型往往对训练数据以及测试数据都有较好的预测。

    贝叶斯估计中的最大后验概率估计(Maximum Posterior Probability Estimation, MAP)是一个结构风险最小化的例子。

**注意**：

这里我们已经将监督学习问题转化为了经验风险或者结构风险函数的最优化问题。此时经验风险函数或结构风险函数是最优化的目标函数。

## （3）算法

上面我们已经将监督学习问题转化为优化问题，那么对于统计学习来讲，“算法”就指的是求解最优化问题的算法了。

如果最优化问题有显式的解析解，这个最优化问题就比较简单；但通常解析解不存在，这就需要数值计算的方法求解。

# 3. 模型选择与模型评估
- 模型选择：选择哪一种学习算法，选择哪一种参数配置；（一般在验证集上进行）
- 模型评价：选择一个（最好）模型后，在新的数据上来评价其预测误差等评价指标。

## （1）训练误差与测试误差
计算误差时需要使用到损失函数，需要注意的是：`统计学习方法所采用的损失函数未必是模型评估时使用的损失函数`。

- **训练误差**的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要；
- **测试误差**反映了学习方法对未知的测试数据集的预测能力（泛化能力），是学习中的重要概念。

## （2）过拟合与模型选择
下图描述了**训练误差**和**测试误差**与**模型复杂度**之间的关系：

![训练误差测试误差与模型复杂度关系](http://i.imgur.com/hL8kvEk.png)


# 4. 正则化
正则化是结构风险最小化策略的实现，它是在经验风险基础上加上模型复杂度的正则化项(Regularizer)或罚项(Penalty Term)。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如正则化项可以是模型参数向量的范数。正则化项一般具有如下形式：

![结构风险最小化](http://i.imgur.com/bWLZrBA.png)

其中第一项为经验风险；`J(ƒ)`为模型的复杂度，是定义在假设空间`ϝ`上的泛函。模型`ƒ`越复杂，复杂度`J(ƒ)`就越大；否则，复杂度越小。`λ>=0`是系数，或者说权重，用来权衡经验风险和模型复杂度。

<br />

正则化项可以取不同的形式。比如回归问题中，损失函数是平方损失，
- 正则化项可以是参数向量`w`的**L2范数**：

    ![l2](http://i.imgur.com/pE6FmKk.png)

- 也可以是参数向量`w`的**L1范数**：

    ![l1](http://i.imgur.com/OmGm3ZJ.png)

<br />

从**贝叶斯估计**的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。

# 5. 泛化能力
## （1）泛化误差
学习方法的**泛化能力**(Generalization Ability)是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。

现实中采用最多的办法是通过测试误差来评价学习方法的泛化能力。但这种评价是依赖于测试数据集的。因为测试数据集有限，得出的评价结果可能不是很可靠。统计学习理论试图从理论上对学习方法的泛化能力进行分析。

如果学到的模型是`ƒ^`,那么用这个模型对未知数据预测的误差即为泛化误差，定义为：

![泛化误差](http://i.imgur.com/G3yfEeG.png)

其实泛化误差就是模型的**期望风险**。

## （2）泛化误差上界
学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的，简称**泛化误差上界**(Generalization Error Bound)。比较两种学习方法的泛化误差是通过比较它们的泛化误差上界进行的。

泛化误差上界通常具有如下性质：
- 它是样本容量的函数，当样本容量增大时，泛化上界趋于0
- 它是假设空间容量的函数，假设空间容量越大，模型越难学，泛化误差上界就越大。

**定义**如下：

对二分类问题，当假设空间是有限个函数的集合`ϝ={ƒ1,ƒ2,...,ƒd}`时，对任意一个函数`ƒ∈ϝ`,至少以概率`1-δ`,以下不等式成立：

![泛化误差上界](http://i.imgur.com/cexarVK.png)

各项含义：
- ![期望风险](http://i.imgur.com/MawslMf.png) **泛化误差**即期望风险
- ![经验风险](http://i.imgur.com/cRhSVfu.png) 代表**经验误差**
- ![Imgur](http://i.imgur.com/WQGETc9.png) 它是`N`的单调递减函数，当`N`趋于无穷时趋于0；同时它也是关于假设空间容量`d`的递增函数，`d`越大，`ε(d,N,δ)`越大。

(具体证明请见《统计学习方法》P16)

以上只讨论了**假设空间包含有限个函数**情况下的泛化误差上界，一般的假设空间要找到泛化误差上界没有这么简单。

# 6. 生成模型和判别模型
监督学习方法又可以分为**生成方法**(Generative Approach)和**判别方法**(Discriminative Approach),所学到的模型分别称为生成模型和判别模型。

**定义**：

- **生成模型**由数据学习联合概率分布`P(X,Y)`,然后求出条件概率分布`P(Y|X)`作为预测的模型，即生成模型：

    ![条件概率](http://i.imgur.com/4rod5vF.png)

    这样的方法之所以被称为生成方法，是因为模型表示了给定输入`X`，产生输出`Y`的生成关系。典型的生成模型有：朴素贝叶斯法和隐马尔科夫模型。

- **判别模型**由数据直接学习决策函数`ƒ(X)`或者条件概率分布`P(Y|X)`作为预测。典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯蒂回归模型、最大熵模型、支持向量机、提升方法和条件随机场等。

**特点**：

- **生成方法的特点**：
    - 生成方法可以还原出联合概率分布`P(X,Y)`,而判别法不能；
    - 生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型；
    - 当存在隐变量时，仍可以用生成方法学习，此时判别方法就不能用。
- **判别方法的特点**：
    - 判别方法直接学习条件概率或者决策函数，学习的准确率更高；
    - 由于直接学习`P(Y|X)`或`ƒ(X)`,可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

# 7. 分类问题
略

# 8. 标注问题
标注(tagging)也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测(structure prediction)问题的简单形式。标注问题的输入是一个观测序列，输出是一个标记序列或者状态序列。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。

**注意**：可能标记的个数是有限的，但其组合所成的标记序列的个数是依序列长度呈指数级增长的。

**过程**：

首先给定一个训练数据集![training set](http://i.imgur.com/BydJWwc.png),这里，![x](http://i.imgur.com/O5uwHzm.png),是输入观测序列；![y](http://i.imgur.com/j7fwFet.png)是相应的输出标记序列，`n`是序列的长度。学习系统基于训练数据集构建一个模型，表示为条件概率分布:

![条件概率](http://i.imgur.com/Q5BvpbA.png)

这里每一个![Imgur](http://i.imgur.com/SukIZ6l.png)取值为所有可能的观测，每一个![Yi](http://i.imgur.com/RkGERRk.png)取值为所有可能的标记，一般`n << N`。

**评价标注模型的指标**与评价分类模型的指标一样，常用的有标注准确率、精确率和召回率。

标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。

# 9. 回归问题
回归是监督学习的另一个重要问题。回归用于预测输入变量(自变量)和输出变量(因变量)之间的关系，特别是当输入变量的值发生变化时，输出变量的值随之发生变化。**回归模型是表示从输入变量到输出变量之间映射的函数**。回归问题的学习等价于函数拟合：选择一条函数曲线使其很好地拟合已知数据且很好地预测未知数据。

回归问题按照输入变量的个数，分为**一元回归**和**多元回归**；按照输入变量和输出变量之间关系的类型即模型的类型，分为**线性回归**和**非线性回归**。

回归学习最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法求解。

