目录：

1. 监督学习
2. 统计学习三要素
    - 模型
    - 策略
    - 算法
3. 模型评估与模型选择
    - 训练误差与测试误差
    - 过拟合与模型选择
4. 正则化
5. 泛化能力
    - 泛化误差
    - 泛化误差上界


# 1. 监督学习

有两点需要**注意**：

1. 监督学习的基本**假设**：随机变量`X`和`Y`具有联合概率分布`P(X,Y)`。
2. 监督学习模型可以是**概率模型**或者**非概率模型**，由条件概率分布`P(Y|X)`或者决策函数`Y=ƒ(X)`表示。预测时，写作：`P(y|x)`或`y=ƒ(x)`。

# 2. 统计学习三要素

统计学习方法都是由模型、策略和算法构成。可以表示为：
```
学习方法 = 模型 + 策略 + 算法
```
## （1）模型
在监督学习过程中，**模型就是所要学习的条件概率分布或决策函数**。模型的假设空间包含所有可能的条件概率分布或决策函数。

假设空间用`Ϝ`表示。
- 假设空间可以定义为**决策函数的集合**：`Ϝ={ƒ|Y=ƒ(X)}`。其中`X`和`Y`分别是定义在输入空间和输出空间上的变量。此时`Ϝ`通常是一个由参数向量决定的**函数簇**：`Ϝ={ƒ|Y=ƒ(X; θ),θ∈R^n}`。向量参数`θ`取值于`n`维欧式空间`R^n`,称为参数空间。
- 假设空间也可以定义为**条件概率的集合**：`Ϝ={P|P(Y|X)}`。其中，`X`和`Y`分别是定义在输入空间和输出空间上的变量。此时`Ϝ`通常是一个由参数向量决定的**条件概率分布簇**：`Ϝ={P|P(Y|X; θ),θ∈R^n}`。向量参数`θ`取值于`n`维欧式空间`R^n`,它也是参数空间。

## （2）策略

策略解决的是**如果选择最优模型**的问题。我们常常有两种策略可以选择：**经验风险最小化策略**和**结构风险最小化策略**。我们先从损失函数谈起：

<br />

**损失函数**

监督学习问题是在假设空间`Ϝ`中选取模型`ƒ`作为决策函数，对于给定的输入`X`，由`ƒ(X)`给出相应的输出`Y'`,预测值`Y'`和真实值`Y`可能一致，也可能不一致，我们用**损失函数**(loss function)或**代价函数**(cost function)来度量预测错误程度。损失函数是`ƒ(X)`和`Y`的非负实值函数，记作`L(Y,ƒ(X))`。

常见的损失函数：
- 0-1损失函数(0-1 loss function)

    ![01lf](http://i.imgur.com/fD0KDOw.png)

- 平方损失函数(quadratic loss function)

    ![qlf](http://i.imgur.com/QkcO76Q.png)

- 绝对损失函数(absolute loss function)

    ![alf](http://i.imgur.com/sDhGZhm.png)

- 对数损失函数(logarithmic loss function)或者对数似然损失函数(log-likelihood loss function)

    ![llf](http://i.imgur.com/laGHvHH.png)

损失函数越小，模型就越好。

<br />

**风险函数**

风险函数(risk function)又称**期望损失**(expected loss),被定义为：

![risk function](http://i.imgur.com/T1BKIHo.png)

含义为：模型`ƒ(X)`关于联合概率分布`P(X,Y)`的平均意义下的损失。

<br />

**经验风险**

期望风险![期望风险](http://i.imgur.com/TcJ7rmv.png)是模型关于联合分布的期望损失，经验风险![经验风险](http://i.imgur.com/txu0oVE.png)是模型关于训练样本集的平均损失。经验风险被定义为：

![经验风险公式](http://i.imgur.com/qkGzSAG.png)

根据大数定理，当样本容量`N`趋于无穷时，经验风险趋于期望风险。所以我们常常用经验风险估计期望风险，但是由于训练样本有限，其效果常常并不理想，可以对经验风险进行一定的矫正。

<br />

**经验风险最小化和结构风险最小化**

- **经验风险最小化**(Expirical Risk Minimization, ERM)的策略认为经验风险最小的模型为最优模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：

    ![经验风险最小化](http://i.imgur.com/tjaoO87.png)

    其中`ϝ`是假设空间。当样本容量足够大时，经验风险最小化能保证有很好的学习效果；但是当样本容量很小时，经验风险最小化学习的效果就不是很好了，容易产生过拟合问题。

    **极大似然估计**(Maximum likelihood estimation)是经验风险最小化的一个例子。

- **结构风险最小化**(Structural Risk Minimization， SRM)是为了防止过拟合而提出来的策略。它等价于正则化(Regularization)。结构风险在经验风险基础上加上模型复杂度的正则化项(Regularizer)或罚项(Penalty Term)。在假设空间、损失函数以及训练数据集确定的情况下，结构风险最小化定义为：

    ![结构风险最小化](http://i.imgur.com/bWLZrBA.png)

    其中`J(ƒ)`为模型的复杂度，是定义在假设空间`ϝ`上的泛函。模型`ƒ`越复杂，复杂度`J(ƒ)`就越大；否则，复杂度越小。`λ>=0`是系数，或者说权重，用来权衡经验风险和模型复杂度。结构风险小需要经验风险和模型复杂度同时小。它训练出的模型往往对训练数据以及测试数据都有较好的预测。

    贝叶斯估计中的最大后验概率估计(Maximum Posterior Probability Estimation, MAP)是一个结构风险最小化的例子。

**注意**：

这里我们已经将监督学习问题转化为了经验风险或者结构风险函数的最优化问题。此时经验风险函数或结构风险函数是最优化的目标函数。

## （3）算法

上面我们已经将监督学习问题转化为优化问题，那么对于统计学习来讲，“算法”就指的是求解最优化问题的算法了。

如果最优化问题有显式的解析解，这个最优化问题就比较简单；但通常解析解不存在，这就需要数值计算的方法求解。

# 3. 模型选择与模型评估
- 模型选择：选择哪一种学习算法，选择哪一种参数配置；（一般在验证集上进行）
- 模型评价：选择一个（最好）模型后，在新的数据上来评价其预测误差等评价指标。

## （1）训练误差与测试误差
计算误差时需要使用到损失函数，需要注意的是：`统计学习方法所采用的损失函数未必是模型评估时使用的损失函数`。

- **训练误差**的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，但本质上不重要；
- **测试误差**反映了学习方法对未知的测试数据集的预测能力（泛化能力），是学习中的重要概念。

## （2）过拟合与模型选择
下图描述了**训练误差**和**测试误差**与**模型复杂度**之间的关系：

![训练误差测试误差与模型复杂度关系](http://i.imgur.com/hL8kvEk.png)


# 4. 正则化
正则化是结构风险最小化策略的实现，它是在经验风险基础上加上模型复杂度的正则化项(Regularizer)或罚项(Penalty Term)。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。比如正则化项可以是模型参数向量的范数。正则化项一般具有如下形式：

![结构风险最小化](http://i.imgur.com/bWLZrBA.png)

其中第一项为经验风险；`J(ƒ)`为模型的复杂度，是定义在假设空间`ϝ`上的泛函。模型`ƒ`越复杂，复杂度`J(ƒ)`就越大；否则，复杂度越小。`λ>=0`是系数，或者说权重，用来权衡经验风险和模型复杂度。

<br />

正则化项可以取不同的形式。比如回归问题中，损失函数是平方损失，
- 正则化项可以是参数向量`w`的**L2范数**：

    ![l2](http://i.imgur.com/pE6FmKk.png)

- 也可以是参数向量`w`的**L1范数**：

    ![l1](http://i.imgur.com/OmGm3ZJ.png)

<br />

从**贝叶斯估计**的角度来看，正则化项对应于模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。

# 5. 泛化能力
## （1）泛化误差
学习方法的**泛化能力**(Generalization Ability)是指由该方法学习到的模型对未知数据的预测能力，是学习方法本质上重要的性质。

现实中采用最多的办法是通过测试误差来评价学习方法的泛化能力。但这种评价是依赖于测试数据集的。因为测试数据集有限，得出的评价结果可能不是很可靠。统计学习理论试图从理论上对学习方法的泛化能力进行分析。

如果学到的模型是`ƒ^`,那么用这个模型对未知数据预测的误差即为泛化误差，定义为：

![泛化误差](http://i.imgur.com/G3yfEeG.png)

其实泛化误差就是模型的**期望风险**。

## （2）泛化误差上界


