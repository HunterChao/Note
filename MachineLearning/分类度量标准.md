目录：

1. Overview
2. 精度（Accuracy score）
3. Cohen’s kappa
4. 混淆矩阵（Confusion atrix）
5. Classification report
6. Hamming loss
7. Jaccard similarity coefficient score
8. Precision, recall and F-measures
9. Hinge loss
10. Log loss
11. Matthews correlation coefficient
12. Receiver operating characteristic (ROC)
13. Zero one loss
14. Brier score loss

# 1. Overview

![ClassifyMetrics](http://i.imgur.com/5u4SGRQ.png)

# 2. 精度（Accuracy score）

使用[`accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)函数计算。

公式：

![Accuracy](http://i.imgur.com/MwSojHZ.png)

这里`1(x)`是指示函数

*Code Example*:
```python
>>> import numpy as np
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)  # 返回精度
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)  # 返回准确个数
2

```

# 3. Cohen’s kappa

科恩kappa统计量用来检验两个观测者在分类问题上的一致性程度。使用[`cohen_kappa_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html#sklearn.metrics.cohen_kappa_score)计算。

The kappa score is a number between `-1` and `1`. Scores above `.8` are generally considered good agreement; zero or lower means no agreement (practically random labels).

*Code Example：*
```python
>>> from sklearn.metrics import cohen_kappa_score
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> cohen_kappa_score(y_true, y_pred)
0.4285714285714286

```

**评价**

Kappa scores can be computed for binary or multiclass problems, but not for multilabel problems and not for more than two annotators.

# 4. Confusion Matrix
使用[`confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)函数。

*Code Example*:
```python
>>> from sklearn.metrics import confusion_matrix
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
       [0, 0, 1],
       [1, 0, 2]])
```
其中：横坐标x是真实，纵坐标y是预测。

对于二分类，可以调用`ravel()`函数获得tn，fp，fn，tp
```python
>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]
>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
>>> tn, fp, fn, tp
(2, 1, 2, 3)
```

# 5. Classification Report
使用[`classification_report`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report)函数直接可以得到准确率，召回率，f1值等所有信息。

*Code Example*:
```python

>>> from sklearn.metrics import classification_report
>>> y_true = [0, 1, 2, 2, 0]
>>> y_pred = [0, 0, 2, 1, 0]
>>> target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
             precision    recall  f1-score   support

    class 0       0.67      1.00      0.80         2
    class 1       0.00      0.00      0.00         1
    class 2       1.00      0.50      0.67         2

avg / total       0.67      0.60      0.59         5

```

# 6. Hamming loss
使用[`hamming_loss`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss)可以获得hamming loss。

（在信息论中，两个等长字符串之间的汉明距离（英语：Hamming distance）是两个字符串对应位置的不同字符的个数）

![hamming loss](http://i.imgur.com/QhUW4ZD.png)

**hamming loss主要使用在multilabel分类中**。在单label分类中，hamming loss和accuracy之和为1。

*Code Example*:
```python
>>> from sklearn.metrics import hamming_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25

>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))  # Mutilabel
0.75
```

# 7. Jaccard similarity coefficient score
[`jaccard_similarity_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_similarity_score.html#sklearn.metrics.jaccard_similarity_score)用来计算**预测label**和**真实lebel**集合的jaccard相似度系数。它被定义为两个label集合的交集比上她们的并集。

*Code Example*:
```python
>>> import numpy as np
>>> from sklearn.metrics import jaccard_similarity_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> jaccard_similarity_score(y_true, y_pred)
0.5
>>> jaccard_similarity_score(y_true, y_pred, normalize=False)
2

>>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.75
```

**注意**：

在单label分类任务中，它等价于accuracy。





